{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528b9886-969c-4dbe-ac14-5c2eab3b196e",
   "metadata": {},
   "source": [
    "# Under and Over Fitting\n",
    "\n",
    "El principal problema del _machine learning_ es el trade off entre la optimización y generalización de los modelos.\n",
    "\n",
    "La optimización es el proceso de ajustar un modelo para conseguir el mejor rendimiento posible con datos de entrenamiento.\n",
    "\n",
    "La generalización es el rendimiento del modelo entrenado con datos que nunca ha visto.\n",
    "\n",
    "El objetivo es conseguir un modelo óptimo con los datos de entrenamiento y, a su vez, que aporte una buena generalización a datos nuevos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392e10d-5098-4967-b11f-124b188046ec",
   "metadata": {},
   "source": [
    "Hay una correlación entre la optimización y la generalización, cuando más bajo es la perdida en los datos de entrenamiento, más bajo será en la prueba. En este caso se dice que el modelo esta sub ajustado (_underfitting_).\n",
    "\n",
    "Luego de varias iteraciones se puede mejorar la perdida en los datos de entrenamiento, pero hay un punto dado en que los datos de prueba dejan de mejorar y la metrica de desempeño empieza a disminuir, en este caso el modelo esta sobre ajustado (_overfitting_). Esto quiere decir que el modelo ha empezado a aprender patrones especificos de los datos entrenamiento, pero son engañosos o irrelevantes para los datos nuevos. En este caso, la mejor solución es conseguir más datos de entrenamiento. Un modelo con más datos generaliza mejor. Cuando no es posible, la siguiente mejor solución consiste en limitar los patrones que puede aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087e590-4e3b-49cf-b85e-49990d0629c3",
   "metadata": {},
   "source": [
    "## Reducir el tamaño de la red\n",
    "\n",
    "La forma mas sencilla de evitar el sobreajuste es reducir el tamaño del modelo: el número de parámetros aprendibles del mismo (dado el número de capas y unidades por capa).\n",
    "\n",
    "El número de parámetros aprendibles de un modelo suele recibir el nombre de \"capacidad\" del modelo, un modelo con más parámetros tiene más capacidad de memorizar.\n",
    "\n",
    "Se debe encontrar un balance en la cantidad de parámetros del modelo para no caer ni en _underfitting_ ni en _overfitting_.\n",
    "\n",
    "En general se comienza con un modelo con pocas capas y parámetros, y se aumenta el tamaño de las capas o se añaden nuevas capas hasta que vemos disminuir las metricas de evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655a74f-d1c5-4dfc-ab88-8a515b79c31e",
   "metadata": {},
   "source": [
    "## Añadir regularizadores de peso\n",
    "\n",
    "Los modelos más sencillos suelen tener menos probabilidades de sobre ajuste que los modelos complejos.\n",
    "\n",
    "En este contexto, un modelo sencillo es aquel que la distribución de los valores de parámetros tienen menos entropía.\n",
    "\n",
    "Así pues, una forma habitual de mitigar el sobre ajuste es limitar la complejidad de la red forzando sus pesos a tomar solo valores pequeños, generando una distribución de pesos más regulares. \n",
    "\n",
    "Esto se logra añadiendo a la función de perdida de la red un coste asociado a tener pesos grandes.\n",
    "\n",
    "En Keras, la regularización de pesos se añade como instancias regularizadoras de pesos a las capas como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686c555-80fc-4367-9649-70bd0b9c205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16,\n",
    "                       kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu',\n",
    "                       input_shape=(10000,)))\n",
    "model.add(layers.Dense(16,\n",
    "                       kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a552d-0185-426d-a06f-9ea89939fa19",
   "metadata": {},
   "source": [
    "l2(0.001) significa que cada coeficiente de la matriz de peso de la capa sumará 0.001 * _weight_coefficient_value_ a la perdida total de la red. Observe que, como se esta penalizando solo se añade durante el entrenamiento, la perdida de esta función será mucho más alta en el entrenamiento que en la prueba.\n",
    "\n",
    "Esto permite hacer más resistente al sobre ajuste.\n",
    "\n",
    "Existen dos tipos de regularizadores: L1 y L2, y se pueden ocupar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67039151-3ad1-4870-84a4-6c859bb83395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "regularizers.l1(0.001)\n",
    "regularizers.l2(0.001)\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3418b-6136-4382-89e4-483a82359ece",
   "metadata": {},
   "source": [
    "## Añadir dropout\n",
    "\n",
    "Es una de las técnicas de regularización más efectivas y más utilizadas para redes neuronales.\n",
    "\n",
    "Aplicarla a una capa consiste en retirar (poner a cero) aleatoriamente un número de características de salida de la capa durante el entrenamiento .\n",
    "\n",
    "Suponiendo que una capa determinada devuelve los valores en vector (0.2, 0.5, 1.3, 0.8, 1.1) para una muestra dada durante el entrenamiento. Al aplicar el _dropout_, este vector tendrá algunas entradas en cero distribuidas al azar: por ejemplo (0, 0.5, 1.3, 0, 1.1).\n",
    "\n",
    "La \"tasa de _dropout_\" es la fracción de las características que se ponen a cero; normalmente entre 0.2 y 0.5.\n",
    "\n",
    "Si bien la solución parece extraña y arbitraria, sin embargo, los desarrolladores comentan que se inspiraron, entre otras cosas, en un mecanismo de prevención de fraudes que utilizan los bancos. Los cajeros suelen rotar bastante entre sucursales, con la finalidad de evitar que los colaboradores cooperen en una posible evento de fraude. Esto hizo pensar a los investigadores que quitar aleatoriamente un subconjunto de diferntes neuronas evitaría la conspiración y reduciría el sobre ajuste.\n",
    "\n",
    "Esto permite romper patrones dados por casualidad que no son significativos.\n",
    "\n",
    "En Keras el _dropout_ se agrega de la manera siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e38af9-b050-44c4-ab28-4d60af08a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2708ad-530b-434c-870c-a0ee95b52709",
   "metadata": {},
   "source": [
    "Se podrían agregar dos capas de _Dropout_ a la red, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cde65-62f7-46bd-a4a9-bf53fd68715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
